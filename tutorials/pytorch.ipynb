{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://www.python-course.eu/neural_network_mnist.php\n",
    "train_data = np.loadtxt('mnist/train.csv', delimiter=',') \n",
    "test_data = np.loadtxt('mnist/test.csv', delimiter=',') \n",
    "\n",
    "trn_X = train_data[:, 1:]\n",
    "trn_y = train_data[:, :1]\n",
    "tst_X = test_data[:, 1:]\n",
    "tst_y = test_data[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  51., 159., 253., 159.,  50.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values range from 0 to 255\n",
    "trn_X[0:5,125:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.2       , 0.62352941, 0.99215686,\n",
       "        0.62352941, 0.19607843, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize data\n",
    "trn_X /= 255\n",
    "tst_X /= 255\n",
    "trn_X[0:5,125:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1105dd6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an image\n",
    "plt.imshow(trn_X[0].reshape(28, 28), cmap='gray')\n",
    "trn_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert NumPy arrays to torch tensors\n",
    "trn_X, trn_y, tst_X, tst_y = map(torch.tensor, (trn_X, trn_y, tst_X, tst_y))\n",
    "\n",
    "trn_X = trn_X.type('torch.FloatTensor')\n",
    "trn_y = trn_y.type('torch.FloatTensor')\n",
    "tst_X = tst_X.type('torch.FloatTensor')\n",
    "tst_y = tst_y.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0022,  0.0227, -0.0308,  ...,  0.0068, -0.0015,  0.0088],\n",
       "        [ 0.0197,  0.0461,  0.0370,  ...,  0.0084, -0.0250, -0.0155],\n",
       "        [-0.0131, -0.0086,  0.0420,  ..., -0.0112, -0.0145, -0.0125],\n",
       "        ...,\n",
       "        [-0.0148, -0.0392,  0.0079,  ..., -0.0910,  0.0313, -0.0857],\n",
       "        [-0.0345, -0.0016, -0.0575,  ...,  0.0095, -0.0774,  0.0024],\n",
       "        [ 0.0261, -0.0645,  0.0528,  ..., -0.0210, -0.0100,  0.0384]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# trailing _'s in PyTorch signifies an in-place operation\n",
    "weights.requires_grad_()\n",
    "# bias: kind of like a coefficient added to each term\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3133, -0.3133])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "log_softmax(torch.tensor([0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5982, 148.4132, 403.4288]])\n",
      "tensor([ 30.1929, 606.4401])\n",
      "tensor([3.4076, 6.4076])\n",
      "tensor([[ 30.1929],\n",
      "        [606.4401]])\n",
      "tensor([[ -29.1929,  -28.1929,  -27.1929],\n",
      "        [-602.4401, -601.4401, -600.4401]])\n"
     ]
    }
   ],
   "source": [
    "# Example to understand log_softmax\n",
    "test_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]).type(torch.FloatTensor)\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.exp())\n",
    "print(test_tensor.exp().sum(-1))\n",
    "print(test_tensor.exp().sum(-1).log())\n",
    "print(test_tensor.exp().sum(-1).unsqueeze(-1))\n",
    "print(test_tensor - test_tensor.exp().sum(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for learning\n",
    "def model(batch):\n",
    "    return log_softmax(batch @ weights + bias)\n",
    "# Note: @ is the dot product operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure dimensions line up\n",
    "trn_X.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.1167, -1.8724, -3.2058, -2.1228, -2.0106, -2.2342, -2.7280, -2.8415,\n",
       "         -2.2118, -2.3925], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batch = trn_X[0:batch_size]\n",
    "predictions = model(batch)\n",
    "predictions[0], predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: the higher, the worse the model performs\n",
    "def neg_log_likelihood(predictions, target):\n",
    "    return -predictions[range(target.shape[0]), target.type(torch.LongTensor)].mean()\n",
    "\n",
    "# gather: gathers values along an axis with a specified tensor as indices\n",
    "# view: reshapes it to the specified shape in each dimension\n",
    "def loss_2(predictions, target):\n",
    "    target_indices = torch.gather(predictions, 1, target.view(-1, 1).type(torch.LongTensor))\n",
    "    return (target_indices).log().mean() / (-1 * predictions.shape[0])\n",
    "    # return (target @ target_indices.log() + \n",
    "    #    ((1 - target) @ (1 - target_indices)).log()).squeeze() / (-1 * predictions.shape[0])\n",
    "    \n",
    "def mean_squared(predictions, target):\n",
    "    target_indices = torch.gather(predictions, 1, target.view(-1, 1).type(torch.LongTensor))\n",
    "    return ((1 - target_indices) ** 2).sum() / predictions.shape[0]\n",
    "\n",
    "loss_function = neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "test_pred = torch.tensor([[0.9, 0.1, 0.5], [0.1, 0.9, 0.5]]).type(torch.FloatTensor)\n",
    "test_target = torch.tensor([[2, 1]]).type(torch.FloatTensor)\n",
    "\n",
    "loss_function(test_pred, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3272, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape, trn_y[0:batch_size].shape)\n",
    "loss_function(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 1])\n",
      "range(0, 2) tensor([[1.],\n",
      "        [2.]])\n",
      "tensor([[-2., -5.],\n",
      "        [-3., -6.]])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Example to understand neg_log_likelihood\n",
    "test_pred = torch.tensor([[-1, -2, -3], [-4, -5, -6]]).type(torch.FloatTensor)\n",
    "test_target = torch.tensor([[1], [2]]).type(torch.FloatTensor)\n",
    "print(test_pred.shape, test_target.shape)\n",
    "print(range(test_target.shape[0]), test_target)\n",
    "print(test_pred[range(test_target.shape[0]), test_target.type(torch.LongTensor)])\n",
    "print(-test_pred[range(test_target.shape[0]), test_target.type(torch.LongTensor)].mean())\n",
    "# Honestly, I don't understand their code for log_softmax and neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy(predictions, target):\n",
    "    preds = torch.argmax(predictions, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1187)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trn_X.shape, weights.shape, bias.shape)\n",
    "# predictions = model(trn_X)\n",
    "# predictions.backward(torch.ones(predictions.shape))\n",
    "# print(weights.grad[:, 4])\n",
    "# weights.grad.zero_()\n",
    "\n",
    "# model_loss = loss_function(model(trn_X), trn_y)\n",
    "# print(model_loss)\n",
    "# model_loss.backward()\n",
    "# print(weights.grad.max())\n",
    "# weights.grad.zero_()\n",
    "# print()\n",
    "\n",
    "# print(loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     weights -= weights.grad * 0.1\n",
    "#     print(weights.grad.max())\n",
    "#     bias -= bias.grad * 0.1\n",
    "#     # Set gradients to zero so gradients are correct for next loop\n",
    "#     weights.grad.zero_()\n",
    "#     bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Run through all the data in batches\n",
    "    for i in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = trn_X[start:end]\n",
    "        batch_y = trn_y[start:end]\n",
    "        predictions = model(batch_X)\n",
    "        model_loss = loss_function(predictions, batch_y)\n",
    "\n",
    "        # Calculate gradients on weights and bias\n",
    "        model_loss.backward()\n",
    "        # Update weights and bias, but don't calculate gradients for this process\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            # Set gradients to zero so gradients are correct for next loop\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0022,  0.0227, -0.0308,  ...,  0.0068, -0.0015,  0.0088],\n",
       "         [ 0.0197,  0.0461,  0.0370,  ...,  0.0084, -0.0250, -0.0155],\n",
       "         [-0.0131, -0.0086,  0.0420,  ..., -0.0112, -0.0145, -0.0125],\n",
       "         ...,\n",
       "         [-0.0148, -0.0392,  0.0079,  ..., -0.0910,  0.0313, -0.0857],\n",
       "         [-0.0345, -0.0016, -0.0575,  ...,  0.0095, -0.0774,  0.0024],\n",
       "         [ 0.0261, -0.0645,  0.0528,  ..., -0.0210, -0.0100,  0.0384]],\n",
       "        requires_grad=True),\n",
       " tensor([-0.0665,  0.1674,  0.1357,  0.0102, -0.0967, -0.1344, -0.0457,  0.0813,\n",
       "         -0.0521,  0.0009], requires_grad=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5944, grad_fn=<NegBackward>), tensor(0.0965))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
