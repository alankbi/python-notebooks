{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied code to load in the data\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)\n",
    "        \n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.19921875, 0.62109375, 0.98828125,\n",
       "        0.62109375, 0.1953125 , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values range from 0 to 255\n",
    "x_train[0:3,125:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104d02e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an image\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert NumPy arrays to torch tensors\n",
    "trn_X, trn_y, tst_X, tst_y = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "# trn_X = trn_X.type('torch.FloatTensor')\n",
    "# trn_y = trn_y.type('torch.FloatTensor')\n",
    "# tst_X = tst_X.type('torch.FloatTensor')\n",
    "# tst_y = tst_y.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0069,  0.0026,  0.0495,  ..., -0.0073,  0.0415,  0.0013],\n",
       "        [-0.0516, -0.0104,  0.0341,  ..., -0.0298, -0.0179, -0.0624],\n",
       "        [-0.0097, -0.0148, -0.0373,  ...,  0.0172, -0.0593, -0.0009],\n",
       "        ...,\n",
       "        [-0.0255,  0.0367, -0.0128,  ...,  0.0091, -0.0792,  0.0526],\n",
       "        [-0.0180, -0.0627, -0.0251,  ..., -0.1057, -0.0212, -0.0188],\n",
       "        [-0.0221,  0.0251, -0.0326,  ...,  0.0263,  0.0216,  0.0528]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# trailing _'s in PyTorch signifies an in-place operation\n",
    "weights.requires_grad_()\n",
    "# bias: kind of like a coefficient added to each term\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3133, -0.3133])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "log_softmax(torch.tensor([0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5982, 148.4132, 403.4288]])\n",
      "tensor([ 30.1929, 606.4401])\n",
      "tensor([3.4076, 6.4076])\n",
      "tensor([[ 30.1929],\n",
      "        [606.4401]])\n",
      "tensor([[ -29.1929,  -28.1929,  -27.1929],\n",
      "        [-602.4401, -601.4401, -600.4401]])\n"
     ]
    }
   ],
   "source": [
    "# Example to understand log_softmax\n",
    "test_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.exp())\n",
    "print(test_tensor.exp().sum(-1))\n",
    "print(test_tensor.exp().sum(-1).log())\n",
    "print(test_tensor.exp().sum(-1).unsqueeze(-1))\n",
    "print(test_tensor - test_tensor.exp().sum(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for learning\n",
    "def model(batch):\n",
    "    return log_softmax(batch @ weights + bias)\n",
    "# Note: @ is the dot product operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([784, 10]), torch.Size([10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure dimensions line up\n",
    "trn_X.shape, weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.2244, -2.3621, -2.3954, -2.1655, -2.1884, -2.2332, -2.5404, -2.2558,\n",
       "         -2.4577, -2.2702], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batch = trn_X[0:batch_size]\n",
    "predictions = model(batch)\n",
    "predictions[0], predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: the higher, the worse the model performs\n",
    "def neg_log_likelihood(predictions, target):\n",
    "    return -predictions[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_function = neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.2985, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape, trn_y[0:batch_size].shape)\n",
    "loss_function(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy(predictions, target):\n",
    "    preds = torch.argmax(predictions, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Run through all the data in batches\n",
    "    for i in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = trn_X[start:end]\n",
    "        batch_y = trn_y[start:end]\n",
    "        predictions = model(batch_X)\n",
    "        model_loss = loss_function(predictions, batch_y)\n",
    "\n",
    "        # Calculate gradients on weights and bias\n",
    "        model_loss.backward()\n",
    "        # Update weights and bias, but don't calculate gradients for this process\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            # Set gradients to zero so gradients are correct for next loop\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0069,  0.0026,  0.0495,  ..., -0.0073,  0.0415,  0.0013],\n",
       "         [-0.0516, -0.0104,  0.0341,  ..., -0.0298, -0.0179, -0.0624],\n",
       "         [-0.0097, -0.0148, -0.0373,  ...,  0.0172, -0.0593, -0.0009],\n",
       "         ...,\n",
       "         [-0.0255,  0.0367, -0.0128,  ...,  0.0091, -0.0792,  0.0526],\n",
       "         [-0.0180, -0.0627, -0.0251,  ..., -0.1057, -0.0212, -0.0188],\n",
       "         [-0.0221,  0.0251, -0.0326,  ...,  0.0263,  0.0216,  0.0528]],\n",
       "        requires_grad=True),\n",
       " tensor([-0.4774,  0.4061,  0.1731, -0.3251,  0.0284,  1.7140, -0.1674,  0.8140,\n",
       "         -1.8164, -0.3494], requires_grad=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2917, grad_fn=<NegBackward>), tensor(0.9180))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print loss and accuracy of trained model\n",
    "loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported as F, torch.nn.functional contains a lot of activation, loss, and other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Cross entropy combines log-softmax and neg-log-likelihood\n",
    "loss_function = F.cross_entropy\n",
    "\n",
    "# Need new model that doesn't use our custom activation function\n",
    "def model_functional(batch):\n",
    "    return batch @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2917, grad_fn=<NllLossBackward>), tensor(0.9180))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss and accuracy should be the same as before\n",
    "loss_function(model_functional(tst_X), tst_y), accuracy(model_functional(tst_X), tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Module allows us to create cleaner code with classes and other stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Using nn.Parameter adds it to the list of returned values from self.parameters()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "        self.learning_rate = 0.5\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Returns weights and bias\n",
    "                    for p in self.parameters():\n",
    "                        p -= p.grad * self.learning_rate\n",
    "                    # Equivalent to doing p.grad.zero_() for each parameter\n",
    "                    self.zero_grad()\n",
    "                   \n",
    "    # Equivalent to (from outside the class): loss_function(model(tst_X), tst_y)\n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    # Equivalent to (from outside the class): accuracy(model(tst_X), tst_y)\n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3304, grad_fn=<NllLossBackward>) tensor(0.0976)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.3304, grad_fn=<NllLossBackward>), tensor(0.0976))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticClassifier()\n",
    "print(loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y))\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2843, -0.1678,  0.3065,  ..., -0.5095, -0.0750, -0.6615],\n",
      "        [-0.0355,  0.1091, -0.2781,  ..., -0.4955, -0.3272, -0.2944],\n",
      "        [ 0.0090, -0.4558,  0.1337,  ..., -0.2369,  0.1668,  0.0087],\n",
      "        ...,\n",
      "        [ 0.2790,  0.0827,  0.0225,  ...,  0.0471,  0.2188,  0.0274],\n",
      "        [ 0.2218, -0.3404,  0.4534,  ..., -0.0184, -0.1911, -0.0554],\n",
      "        [ 0.5538, -0.3151, -0.0921,  ..., -0.0115, -0.3094, -0.1018]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2843, -0.1678,  0.3065,  ..., -0.5095, -0.0750, -0.6615],\n",
       "        [-0.0355,  0.1091, -0.2781,  ..., -0.4955, -0.3272, -0.2944],\n",
       "        [ 0.0090, -0.4558,  0.1337,  ..., -0.2369,  0.1668,  0.0087],\n",
       "        ...,\n",
       "        [ 0.2790,  0.0827,  0.0225,  ...,  0.0471,  0.2188,  0.0274],\n",
       "        [ 0.2218, -0.3404,  0.4534,  ..., -0.0184, -0.1911, -0.0554],\n",
       "        [ 0.5538, -0.3151, -0.0921,  ..., -0.0115, -0.3094, -0.1018]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Although model is an object, you can call it as if it were a function\n",
    "# Behind the scenes, running model() calls the forward method automatically\n",
    "print(model(tst_X))\n",
    "model.forward(tst_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2927, grad_fn=<NllLossBackward>), tensor(0.9179))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear creates a linear layer for us, so we don't need to manually define weights/bias or the feed-forward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All that has changed are the constructor and the fit method\n",
    "class LinearLogisticClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "        self.learning_rate = 0.5\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Calls nn.Linear.forward method\n",
    "        return self.linear(X)\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # self.parameters() contains self.linear.weight and self.linear.bias\n",
    "                    for p in self.parameters():\n",
    "                        p -= p.grad * self.learning_rate\n",
    "                    self.zero_grad()\n",
    "                   \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3434, grad_fn=<NllLossBackward>), tensor(0.1141))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearLogisticClassifier()\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2921, grad_fn=<NllLossBackward>), tensor(0.9183))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With torch.optim, we no longer need to manually update our parameters and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class OptLinearLogisticClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "        # Create optim object for our Linear object's parameters (Note: lr = learning_rate)\n",
    "        self.opt = optim.SGD(self.parameters(), lr=0.5)\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "\n",
    "                # Performs the updates to the parameters\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "                   \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3050, grad_fn=<NllLossBackward>), tensor(0.1168))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OptLinearLogisticClassifier()\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2925, grad_fn=<NllLossBackward>), tensor(0.9177))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorDatasets provide an easier way to index over our features and label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train = TensorDataset(trn_X, trn_y)\n",
    "print(trn_X[0:2], trn_y[0:2])\n",
    "train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([3, 5]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X, batch_y = train[10:12]\n",
    "batch_X, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders help you load your data in batches instead of manually having to loop through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "class DataLoaderClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        self.opt = optim.SGD(self.parameters(), lr=0.5)\n",
    "        self.loss_function = F.cross_entropy\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64, epochs=2):\n",
    "        train = TensorDataset(trn_X, trn_y)\n",
    "        # Always good to shuffle training data\n",
    "        loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "                   \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3122, grad_fn=<NllLossBackward>) tensor(0.1192)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.2993, grad_fn=<NllLossBackward>), tensor(0.9148))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DataLoaderClassifier()\n",
    "print(model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y))\n",
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.train() and model.eval() set whether in training or validation mode\n",
    "# Inside your fit method or class, can change behavior based on self.training (Bool)\n",
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)\n",
    "model.train()\n",
    "model.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful guide to understanding CNN's: http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: with convolutional layers, the width and height of the output layer is the following:  \n",
    "`(W - F + 2 * P) / S + 1`  \n",
    "Where W = input dimensions, F = filter size, P = zero padding, and S = stride size.  \n",
    "e.g. 7x7 input matrix with a 3x3 filter, stride of 1, and no zero padding\n",
    "(7 - 3 + 2 * 0) / 1 + 1 = 5x5 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)\n",
    "        in_channels = depth of input (e.g. RGB images would have three channels)\n",
    "        out_channels = depth of output (i.e. number of filters)\n",
    "        kernel_size = size of filters (e.g. kernel_size=3 produces a 3x3 filter, (2, 3) a 2x3 filter)\n",
    "        stride = how many indices to \"slide\" when convolving/doing dot products of input layer with filters\n",
    "        padding = number of rows/cols of 0's to pad the input layer with (along width and height dims)\n",
    "        \n",
    "        Visualize parameters: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "        \n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, 3, stride=2, padding=1)\n",
    "        \n",
    "        # Momentum: increases converge rates using the physics idea of rolling down a hill\n",
    "        # Explanation: http://cs231n.github.io/neural-networks-3/#sgd and CTRL+F \"momentum\"\n",
    "        self.opt = optim.SGD(self.parameters(), lr=0.1, momentum=0.9)\n",
    "        \n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "        \n",
    "    def forward(self, X):        \n",
    "        # Reshape X to be one column of 28x28 images\n",
    "        # X's shape is (number of rows, depth, width, height)\n",
    "        X = X.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # ReLU: set X = max(0, X)\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.relu(self.conv3(X))\n",
    "        \n",
    "        # Pooling layer; 4 is the kernel_size parameter\n",
    "        X = F.max_pool2d(X, 4)\n",
    "\n",
    "        # Convert X from 4D tensor back to 2D\n",
    "        return X.view(-1, X.size(1))\n",
    "    \n",
    "    def fit(self, X, y, batch_size=64):\n",
    "        train = TensorDataset(X, y)\n",
    "        loader = DataLoader(train, batch_size=batch_size)\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.opt.zero_grad()\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                self.opt.step()\n",
    "            \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 16, 14, 14])\n",
      "torch.Size([64, 16, 7, 7])\n",
      "torch.Size([64, 10, 4, 4])\n",
      "torch.Size([64, 10, 1, 1])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Visualize transformation of X through CNN: \n",
    "\n",
    "X = trn_X[0:64]\n",
    "model = CNN()\n",
    "print(X.shape)\n",
    "        \n",
    "X = X.view(-1, 1, 28, 28)\n",
    "print(X.shape)\n",
    "\n",
    "X = F.relu(model.conv1(X))\n",
    "print(X.shape)\n",
    "\n",
    "X = F.relu(model.conv2(X))\n",
    "print(X.shape)\n",
    "\n",
    "X = F.relu(model.conv3(X))\n",
    "print(X.shape)\n",
    "\n",
    "X = F.max_pool2d(X, 4)\n",
    "print(X.shape)\n",
    "\n",
    "X = X.view(-1, X.size(1))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3043, grad_fn=<NllLossBackward>) tensor(0.0870)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.2844, grad_fn=<NllLossBackward>), tensor(0.9105))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y))\n",
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifies the process of having multiple layers in your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to define custom action for Sequential\n",
    "class Lambda(nn.Module):\n",
    "    \n",
    "    def __init__(self, function):\n",
    "        super().__init__()\n",
    "        self.function = function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy methods to use as utility functions\n",
    "def loss(model, X, y):\n",
    "    return F.cross_entropy(model.forward(X), y)\n",
    "\n",
    "def acc(model, X, y):\n",
    "    return accuracy(model.forward(X), y)\n",
    "\n",
    "def fit(model, X, y, batch_size=64, learning_rate=0.1, epochs=2):\n",
    "    opt = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    train = TensorDataset(X, y)\n",
    "    loader = DataLoader(train, batch_size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in loader:\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss_val = loss(model, batch_X, batch_y)\n",
    "            loss_val.backward()\n",
    "\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3035, grad_fn=<NllLossBackward>) tensor(0.1011)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5012, grad_fn=<NllLossBackward>), tensor(0.8387))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, 3, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, 3, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, 3, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(4),\n",
    "    # Note: we can also use the following line to pool the results to a size of 1x1\n",
    "    # nn.AdaptiveMaxPool2d(1)\n",
    "    # Where the argument 1 specifies the size of the output tensor\n",
    "    Lambda(lambda x: x.view(-1, x.size(1)))\n",
    ")\n",
    "\n",
    "print(loss(model, tst_X, tst_y), acc(model, tst_X, tst_y))\n",
    "fit(model, trn_X, trn_y)\n",
    "loss(model, tst_X, tst_y), acc(model, tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a CUDA-capable GPU, you can run tasks on there to speed them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways to move a tensor to GPU\n",
    "cuda_tensor = torch.ones(5, device=device)  # Most efficient\n",
    "cuda_tensor = cuda_tensor.to(device)  # Most convenient\n",
    "# cuda_tensor = cuda_tensor.cuda()  # Less useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can send both input tensors and models to GPU\n",
    "model = model.to(device)\n",
    "trn_X, trn_y = trn_X.to(device), trn_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
