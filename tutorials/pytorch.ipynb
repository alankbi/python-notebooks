{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://www.python-course.eu/neural_network_mnist.php\n",
    "train_data = np.loadtxt('mnist/train.csv', delimiter=',') \n",
    "test_data = np.loadtxt('mnist/test.csv', delimiter=',') \n",
    "\n",
    "trn_X = train_data[:, 1:]\n",
    "trn_y = train_data[:, :1]\n",
    "tst_X = test_data[:, 1:]\n",
    "tst_y = test_data[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  51., 159., 253., 159.,  50.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values range from 0 to 255\n",
    "trn_X[0:5,125:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15ac6a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an image\n",
    "plt.imshow(trn_X[0].reshape(28, 28), cmap='gray')\n",
    "trn_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert NumPy arrays to torch tensors\n",
    "trn_X, trn_y, tst_X, tst_y = map(torch.tensor, (trn_X, trn_y, tst_X, tst_y))\n",
    "\n",
    "trn_X = trn_X.type('torch.FloatTensor')\n",
    "trn_y = trn_y.type('torch.FloatTensor')\n",
    "tst_X = tst_X.type('torch.FloatTensor')\n",
    "tst_y = tst_y.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0131,  0.0550,  0.0669,  ...,  0.0438,  0.0040,  0.0511],\n",
       "        [ 0.0107, -0.0551, -0.0221,  ..., -0.0093,  0.0449, -0.0167],\n",
       "        [ 0.0151, -0.0089,  0.0233,  ..., -0.0076, -0.0173,  0.0570],\n",
       "        ...,\n",
       "        [-0.0170, -0.0657, -0.0002,  ..., -0.0210,  0.0430, -0.0179],\n",
       "        [-0.0001,  0.0762,  0.0102,  ...,  0.0157,  0.0332,  0.0038],\n",
       "        [-0.0398, -0.0218,  0.0267,  ...,  0.0211,  0.0179,  0.0235]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# trailing _'s in PyTorch signifies an in-place operation\n",
    "weights.requires_grad_()\n",
    "# bias: kind of like a coefficient added to each term\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3133, -0.3133])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "log_softmax(torch.tensor([0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + (-x).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5982, 148.4132, 403.4288]])\n",
      "tensor([ 30.1929, 606.4401])\n",
      "tensor([3.4076, 6.4076])\n",
      "tensor([[ 30.1929],\n",
      "        [606.4401]])\n",
      "tensor([[ -29.1929,  -28.1929,  -27.1929],\n",
      "        [-602.4401, -601.4401, -600.4401]])\n"
     ]
    }
   ],
   "source": [
    "# Example to understand log_softmax\n",
    "test_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]).type(torch.FloatTensor)\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.exp())\n",
    "print(test_tensor.exp().sum(-1))\n",
    "print(test_tensor.exp().sum(-1).log())\n",
    "print(test_tensor.exp().sum(-1).unsqueeze(-1))\n",
    "print(test_tensor - test_tensor.exp().sum(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for learning\n",
    "def model_old(batch):\n",
    "    return log_softmax(batch @ weights + bias)\n",
    "# Note: @ is the dot product operation\n",
    "\n",
    "def model(batch):\n",
    "    return sigmoid(batch @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure dimensions line up\n",
    "trn_X.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 6.3396e-08, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          9.9993e-01, 1.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 5.0794e-24, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 4.7604e-01, 5.6044e-15, 2.0962e-12],\n",
       "         [0.0000e+00, 1.0676e-21, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 1.9617e-05, 9.9449e-01, 9.8425e-01],\n",
       "         [0.0000e+00, 6.5201e-22, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          6.7457e-11, 9.9995e-01, 1.4409e-09, 1.0103e-24],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00, 9.9047e-01, 1.0000e+00, 0.0000e+00]],\n",
       "        grad_fn=<SliceBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batch = trn_X[0:batch_size]\n",
    "predictions = model(batch)\n",
    "predictions[5:10], predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: the higher, the worse the model performs\n",
    "def neg_log_likelihood(predictions, target):\n",
    "    return -predictions[range(target.shape[0]), target.type(torch.LongTensor)].mean()\n",
    "\n",
    "# gather: gathers values along an axis with a specified tensor as indices\n",
    "# view: reshapes it to the specified shape in each dimension\n",
    "def loss_2(predictions, target):\n",
    "    target_indices = torch.gather(predictions, 1, target.view(-1, 1).type(torch.LongTensor))\n",
    "    return (target_indices + 0.01).log().mean() / (-1 * predictions.shape[0])\n",
    "    # return (target @ target_indices.log() + \n",
    "    #    ((1 - target) @ (1 - target_indices)).log()).squeeze() / (-1 * predictions.shape[0])\n",
    "    \n",
    "def mean_squared(predictions, target):\n",
    "    target_indices = torch.gather(predictions, 1, target.view(-1, 1).type(torch.LongTensor))\n",
    "    return ((1 - target_indices) ** 2).sum() / predictions.shape[0]\n",
    "\n",
    "loss_function = mean_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1300)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "test_pred = torch.tensor([[0.9, 0.1, 0.5], [0.1, 0.9, 0.5]]).type(torch.FloatTensor)\n",
    "test_target = torch.tensor([[2, 1]]).type(torch.FloatTensor)\n",
    "\n",
    "loss_function(test_pred, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6875, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape, trn_y[0:batch_size].shape)\n",
    "loss_function(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 1])\n",
      "range(0, 2) tensor([[1.],\n",
      "        [2.]])\n",
      "tensor([[-2., -5.],\n",
      "        [-3., -6.]])\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Example to understand neg_log_likelihood\n",
    "test_pred = torch.tensor([[-1, -2, -3], [-4, -5, -6]]).type(torch.FloatTensor)\n",
    "test_target = torch.tensor([[1], [2]]).type(torch.FloatTensor)\n",
    "print(test_pred.shape, test_target.shape)\n",
    "print(range(test_target.shape[0]), test_target)\n",
    "print(test_pred[range(test_target.shape[0]), test_target.type(torch.LongTensor)])\n",
    "print(-test_pred[range(test_target.shape[0]), test_target.type(torch.LongTensor)].mean())\n",
    "# Honestly, I don't understand their code for log_softmax and neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy(predictions, target):\n",
    "    preds = torch.argmax(predictions, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0886)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Run through all the data in batches\n",
    "        for i in [0]:#range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_X = trn_X[start:end]\n",
    "            batch_y = trn_y[start:end]\n",
    "            predictions = model(batch_X)\n",
    "            model_loss = loss_function(predictions, batch_y)\n",
    "            \n",
    "            # Calculate gradients on weights and bias\n",
    "            model_loss.backward()\n",
    "            # Update weights and bias, but don't calculate gradients for this process\n",
    "            with torch.no_grad():\n",
    "                global weights, bias\n",
    "                weights -= weights.grad * learning_rate\n",
    "                bias -= bias.grad * learning_rate\n",
    "                # Set gradients to zero so gradients are correct for next loop\n",
    "                weights.grad.zero_()\n",
    "                bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0131,  0.0550,  0.0669,  ...,  0.0438,  0.0040,  0.0511],\n",
       "         [ 0.0107, -0.0551, -0.0221,  ..., -0.0093,  0.0449, -0.0167],\n",
       "         [ 0.0151, -0.0089,  0.0233,  ..., -0.0076, -0.0173,  0.0570],\n",
       "         ...,\n",
       "         [-0.0170, -0.0657, -0.0002,  ..., -0.0210,  0.0430, -0.0179],\n",
       "         [-0.0001,  0.0762,  0.0102,  ...,  0.0157,  0.0332,  0.0038],\n",
       "         [-0.0398, -0.0218,  0.0267,  ...,  0.0211,  0.0179,  0.0235]],\n",
       "        requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    nan,     nan,     nan,  ...,     nan,     nan,  0.0511],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan, -0.0167],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,  0.0570],\n",
       "         ...,\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan, -0.0179],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,  0.0038],\n",
       "         [    nan,     nan,     nan,  ...,     nan,     nan,  0.0235]],\n",
       "        requires_grad=True),\n",
       " tensor([       nan,        nan,        nan,        nan, 0.0000e+00,        nan,\n",
       "         0.0000e+00,        nan,        nan, 1.5091e-09], requires_grad=True))"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
