{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied code to load in the data\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)\n",
    "        \n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.19921875, 0.62109375, 0.98828125,\n",
       "        0.62109375, 0.1953125 , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values range from 0 to 255\n",
    "x_train[0:3,125:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b6342a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show an image\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap='gray')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert NumPy arrays to torch tensors\n",
    "trn_X, trn_y, tst_X, tst_y = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "# trn_X = trn_X.type('torch.FloatTensor')\n",
    "# trn_y = trn_y.type('torch.FloatTensor')\n",
    "# tst_X = tst_X.type('torch.FloatTensor')\n",
    "# tst_y = tst_y.type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trn_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0332,  0.0048, -0.0641,  ..., -0.0322,  0.0235,  0.0428],\n",
       "        [ 0.0143, -0.0094,  0.0057,  ...,  0.0432,  0.0010,  0.0522],\n",
       "        [ 0.0291,  0.0356,  0.0768,  ...,  0.0067, -0.0169,  0.0313],\n",
       "        ...,\n",
       "        [ 0.0210,  0.0206,  0.0487,  ..., -0.0180,  0.0284, -0.0210],\n",
       "        [ 0.0946, -0.0036,  0.0168,  ...,  0.0292,  0.0389, -0.0318],\n",
       "        [ 0.0014,  0.0417, -0.0762,  ..., -0.0224,  0.0257, -0.0012]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# trailing _'s in PyTorch signifies an in-place operation\n",
    "weights.requires_grad_()\n",
    "# bias: kind of like a coefficient added to each term\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3133, -0.3133])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "log_softmax(torch.tensor([0.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[  2.7183,   7.3891,  20.0855],\n",
      "        [ 54.5982, 148.4132, 403.4288]])\n",
      "tensor([ 30.1929, 606.4401])\n",
      "tensor([3.4076, 6.4076])\n",
      "tensor([[ 30.1929],\n",
      "        [606.4401]])\n",
      "tensor([[ -29.1929,  -28.1929,  -27.1929],\n",
      "        [-602.4401, -601.4401, -600.4401]])\n"
     ]
    }
   ],
   "source": [
    "# Example to understand log_softmax\n",
    "test_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "print(test_tensor.shape)\n",
    "print(test_tensor.exp())\n",
    "print(test_tensor.exp().sum(-1))\n",
    "print(test_tensor.exp().sum(-1).log())\n",
    "print(test_tensor.exp().sum(-1).unsqueeze(-1))\n",
    "print(test_tensor - test_tensor.exp().sum(-1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for learning\n",
    "def model(batch):\n",
    "    return log_softmax(batch @ weights + bias)\n",
    "# Note: @ is the dot product operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([784, 10]), torch.Size([10]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure dimensions line up\n",
    "trn_X.shape, weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.5336, -1.9068, -2.4705, -2.3072, -2.8452, -2.5241, -2.0376, -2.6237,\n",
       "         -1.8578, -2.3987], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "batch = trn_X[0:batch_size]\n",
    "predictions = model(batch)\n",
    "predictions[0], predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: the higher, the worse the model performs\n",
    "def neg_log_likelihood(predictions, target):\n",
    "    return -predictions[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_function = neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10]) torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3128, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape, trn_y[0:batch_size].shape)\n",
    "loss_function(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy(predictions, target):\n",
    "    preds = torch.argmax(predictions, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1406)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, trn_y[0:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learning_rate = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Run through all the data in batches\n",
    "    for i in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = trn_X[start:end]\n",
    "        batch_y = trn_y[start:end]\n",
    "        predictions = model(batch_X)\n",
    "        model_loss = loss_function(predictions, batch_y)\n",
    "\n",
    "        # Calculate gradients on weights and bias\n",
    "        model_loss.backward()\n",
    "        # Update weights and bias, but don't calculate gradients for this process\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            # Set gradients to zero so gradients are correct for next loop\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0332,  0.0048, -0.0641,  ..., -0.0322,  0.0235,  0.0428],\n",
       "         [ 0.0143, -0.0094,  0.0057,  ...,  0.0432,  0.0010,  0.0522],\n",
       "         [ 0.0291,  0.0356,  0.0768,  ...,  0.0067, -0.0169,  0.0313],\n",
       "         ...,\n",
       "         [ 0.0210,  0.0206,  0.0487,  ..., -0.0180,  0.0284, -0.0210],\n",
       "         [ 0.0946, -0.0036,  0.0168,  ...,  0.0292,  0.0389, -0.0318],\n",
       "         [ 0.0014,  0.0417, -0.0762,  ..., -0.0224,  0.0257, -0.0012]],\n",
       "        requires_grad=True),\n",
       " tensor([-0.4629,  0.4014,  0.1777, -0.3172,  0.0349,  1.7024, -0.1711,  0.8172,\n",
       "         -1.8295, -0.3528], requires_grad=True))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2926, grad_fn=<NegBackward>), tensor(0.9181))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print loss and accuracy of trained model\n",
    "loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported as F, torch.nn.functional contains a lot of activation, loss, and other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Cross entropy combines log-softmax and neg-log-likelihood\n",
    "loss_function = F.cross_entropy\n",
    "\n",
    "# Need new model that doesn't use our custom activation function\n",
    "def model_functional(batch):\n",
    "    return batch @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2926, grad_fn=<NllLossBackward>), tensor(0.9181))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss and accuracy should be the same as before\n",
    "loss_function(model_functional(tst_X), tst_y), accuracy(model_functional(tst_X), tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Module allows us to create cleaner code with classes and other stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Using nn.Parameter adds it to the list of returned values from self.parameters()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "        self.learning_rate = 0.5\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Returns weights and bias\n",
    "                    for p in self.parameters():\n",
    "                        p -= p.grad * self.learning_rate\n",
    "                    # Equivalent to doing p.grad.zero_() for each parameter\n",
    "                    self.zero_grad()\n",
    "                   \n",
    "    # Equivalent to (from outside the class): loss_function(model(tst_X), tst_y)\n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    # Equivalent to (from outside the class): accuracy(model(tst_X), tst_y)\n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3045, grad_fn=<NllLossBackward>) tensor(0.1171)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.3045, grad_fn=<NllLossBackward>), tensor(0.1171))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticClassifier()\n",
    "print(loss_function(model(tst_X), tst_y), accuracy(model(tst_X), tst_y))\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2259e-01, -4.6998e-01,  9.0085e-04,  ..., -1.9788e-01,\n",
      "          5.5158e-01,  9.9997e-01],\n",
      "        [ 4.6301e-01, -6.2655e-01, -7.2549e-02,  ...,  4.8879e-02,\n",
      "          3.2107e-01,  5.7213e-01],\n",
      "        [ 2.4476e-01, -1.4507e-01, -5.8494e-02,  ...,  1.9723e-01,\n",
      "          2.6748e-01,  6.8730e-02],\n",
      "        ...,\n",
      "        [-5.7496e-02, -8.7025e-01, -2.8125e-01,  ...,  3.9468e-01,\n",
      "         -3.7351e-03,  6.5323e-01],\n",
      "        [-5.8419e-02, -2.9478e-01,  6.9313e-02,  ..., -2.9299e-02,\n",
      "         -2.9120e-01, -5.9458e-02],\n",
      "        [-1.2289e-01, -2.6857e-01, -3.7178e-01,  ...,  1.0253e-01,\n",
      "         -1.1414e-01,  3.6279e-02]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2259e-01, -4.6998e-01,  9.0085e-04,  ..., -1.9788e-01,\n",
       "          5.5158e-01,  9.9997e-01],\n",
       "        [ 4.6301e-01, -6.2655e-01, -7.2549e-02,  ...,  4.8879e-02,\n",
       "          3.2107e-01,  5.7213e-01],\n",
       "        [ 2.4476e-01, -1.4507e-01, -5.8494e-02,  ...,  1.9723e-01,\n",
       "          2.6748e-01,  6.8730e-02],\n",
       "        ...,\n",
       "        [-5.7496e-02, -8.7025e-01, -2.8125e-01,  ...,  3.9468e-01,\n",
       "         -3.7351e-03,  6.5323e-01],\n",
       "        [-5.8419e-02, -2.9478e-01,  6.9313e-02,  ..., -2.9299e-02,\n",
       "         -2.9120e-01, -5.9458e-02],\n",
       "        [-1.2289e-01, -2.6857e-01, -3.7178e-01,  ...,  1.0253e-01,\n",
       "         -1.1414e-01,  3.6279e-02]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Although model is an object, you can call it as if it were a function\n",
    "# Behind the scenes, running model() calls the forward method automatically\n",
    "print(model(tst_X))\n",
    "model.forward(tst_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2925, grad_fn=<NllLossBackward>), tensor(0.9170))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear creates a linear layer for us, so we don't need to manually define weights/bias or the feed-forward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All that has changed are the constructor and the fit method\n",
    "class LinearLogisticClassifier(LogisticClassifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "        self.learning_rate = 0.5\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Calls nn.Linear.forward method\n",
    "        return self.linear(X)\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # self.linear.parameters() contains .weight and .bias\n",
    "                    for p in self.linear.parameters():\n",
    "                        p -= p.grad * self.learning_rate\n",
    "                    self.linear.zero_grad()\n",
    "                   \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3150, grad_fn=<NllLossBackward>), tensor(0.1291))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearLogisticClassifier()\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2929, grad_fn=<NllLossBackward>), tensor(0.9181))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With torch.optim, we no longer need to manually update our parameters and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class OptLinearLogisticClassifier(LogisticClassifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "        \n",
    "        # Create optim object for our Linear object's parameters (Note: lr = learning_rate)\n",
    "        self.opt = optim.SGD(self.linear.parameters(), lr=0.5)\n",
    "        self.epochs = 2\n",
    "        self.loss_function = F.cross_entropy\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "    \n",
    "    def fit(self, trn_X, trn_y, batch_size=64):\n",
    "        for i in range(self.epochs):\n",
    "            for j in range((trn_X.shape[0] - 1) // batch_size + 1):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_X = trn_X[start:end]\n",
    "                batch_y = trn_y[start:end]\n",
    "                \n",
    "                loss = self.loss(batch_X, batch_y)\n",
    "                loss.backward()\n",
    "\n",
    "                # Performs the updates to the parameters\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "                   \n",
    "    def loss(self, X, y):\n",
    "        return self.loss_function(self.forward(X), y)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        return accuracy(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3321, grad_fn=<NllLossBackward>), tensor(0.0697))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OptLinearLogisticClassifier()\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2925, grad_fn=<NllLossBackward>), tensor(0.9182))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_X, trn_y)\n",
    "model.loss(tst_X, tst_y), model.accuracy(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorDatasets provide an easier way to index over our features and label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0]))"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train = TensorDataset(trn_X, trn_y)\n",
    "print(trn_X[0:2], trn_y[0:2])\n",
    "train[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([3, 5]))"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X, batch_y = train[10:12]\n",
    "batch_X, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders help you load your data in batches instead of manually having to loop through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
